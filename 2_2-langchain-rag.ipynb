{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rcginne/ML-foundations/blob/master/2_2-langchain-rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a28162-8f3e-42f8-83f6-1b9ef38cd394",
      "metadata": {
        "id": "a2a28162-8f3e-42f8-83f6-1b9ef38cd394"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/vanderbilt-data-science/ai_summer/blob/main/2_2-langchain-rag.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "# AI Solutions with Langchain and RAG\n",
        "> For Vanderbilt University AI Summer 2024<br>Prepared by Dr. Charreau Bell\n",
        "\n",
        "_Code versions applicable: May 14, 2024_\n",
        "\n",
        "## Learning Outcomes:\n",
        "* Participants will be able to articulate the essential steps and components of a retrieval-augmented generation (RAG) system and implement a standard RAG system using langchain.\n",
        "* Participants will gain familiarity in inspecting the execution pathways of LLM-based systems.\n",
        "* Participants will gain familiarity in approaches for the evaluation of LLM-based systems."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c1cebcf-1ac1-48aa-b998-9f0b2a77567f",
      "metadata": {
        "id": "8c1cebcf-1ac1-48aa-b998-9f0b2a77567f"
      },
      "source": [
        "### Computing Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "15202e4f-5a92-4231-a557-43daa913beb5",
      "metadata": {
        "id": "15202e4f-5a92-4231-a557-43daa913beb5",
        "outputId": "5ccdcb48-7ec8-488b-daca-e9e4481793aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.1.20 in /usr/local/lib/python3.12/dist-packages (0.1.20)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.12/dist-packages (0.1.7)\n",
            "Requirement already satisfied: grandalf in /usr/local/lib/python3.12/dist-packages (0.8)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (6.0.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (2.0.43)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (3.12.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (0.6.7)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (0.0.38)\n",
            "Collecting langchain-core<0.2.0,>=0.1.52 (from langchain==0.1.20)\n",
            "  Using cached langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (0.0.2)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.1.20)\n",
            "  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (2.11.9)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (8.5.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from grandalf) (3.2.5)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.20) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.20) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain==0.1.20) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.1.20) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.1.20) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.1.20) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.1.20) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.1.20) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.1.20) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.1.20) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.20) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain==0.1.20) (3.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.20) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Using cached langchain_core-0.1.53-py3-none-any.whl (303 kB)\n",
            "Using cached langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "Installing collected packages: langsmith, langchain-core\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.4.33\n",
            "    Uninstalling langsmith-0.4.33:\n",
            "      Successfully uninstalled langsmith-0.4.33\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.78\n",
            "    Uninstalling langchain-core-0.3.78:\n",
            "      Successfully uninstalled langchain-core-0.3.78\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 2.1.12 requires langchain-core>=0.3.75, but you have langchain-core 0.1.53 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-0.1.53 langsmith-0.1.147\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "langchain_core",
                  "langsmith"
                ]
              },
              "id": "0577979129504a6a8c3bd73c73b22a22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.1.1)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.9)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.37.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.26.4)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.19.2)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.35.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.10)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain==0.1.20 langchain_openai grandalf sentence-transformers\n",
        "! pip install pypdf chromadb faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U langchain-google-genai"
      ],
      "metadata": {
        "id": "SwYUn8iJ_HRd",
        "outputId": "928dfd57-c68d-4895-fa48-57d28044f1cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "SwYUn8iJ_HRd",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (2.1.12)\n",
            "Collecting langchain-core>=0.3.75 (from langchain-google-genai)\n",
            "  Using cached langchain_core-0.3.78-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.11.9)\n",
            "Requirement already satisfied: filetype<2,>=1.2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.29.5)\n",
            "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core>=0.3.75->langchain-google-genai)\n",
            "  Using cached langsmith-0.4.33-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.32.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.3.75->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.3.1)\n",
            "Using cached langchain_core-0.3.78-py3-none-any.whl (449 kB)\n",
            "Using cached langsmith-0.4.33-py3-none-any.whl (387 kB)\n",
            "Installing collected packages: langsmith, langchain-core\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.1.147\n",
            "    Uninstalling langsmith-0.1.147:\n",
            "      Successfully uninstalled langsmith-0.1.147\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.1.53\n",
            "    Uninstalling langchain-core-0.1.53:\n",
            "      Successfully uninstalled langchain-core-0.1.53\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-openai 0.1.7 requires langchain-core<0.3,>=0.1.46, but you have langchain-core 0.3.78 which is incompatible.\n",
            "langchain 0.1.20 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.3.78 which is incompatible.\n",
            "langchain 0.1.20 requires langsmith<0.2.0,>=0.1.17, but you have langsmith 0.4.33 which is incompatible.\n",
            "langchain-text-splitters 0.0.2 requires langchain-core<0.3,>=0.1.28, but you have langchain-core 0.3.78 which is incompatible.\n",
            "langchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.3.78 which is incompatible.\n",
            "langchain-community 0.0.38 requires langsmith<0.2.0,>=0.1.0, but you have langsmith 0.4.33 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-0.3.78 langsmith-0.4.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dd8f69f2",
      "metadata": {
        "id": "dd8f69f2"
      },
      "outputs": [],
      "source": [
        "# Best practice is to do all imports at the beginning of the notebook, but we have separated them here for learning purposes.\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e0e06178",
      "metadata": {
        "id": "e0e06178"
      },
      "outputs": [],
      "source": [
        "# auth replicated here for reference just in case you choose to do something similar\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88934c0a-c340-4678-be23-563a4fe331f1",
      "metadata": {
        "id": "88934c0a-c340-4678-be23-563a4fe331f1"
      },
      "source": [
        "## Langchain Introduction\n",
        "\n",
        "### Overview of System\n",
        "\n",
        "[Overview of Langchain](https://python.langchain.com/v0.1/docs/get_started/introduction/)\n",
        "\n",
        "<figure>\n",
        "<img src='https://python.langchain.com/v0.1/svg/langchain_stack.svg' height=600/>\n",
        "    <figcaption>\n",
        "        Langchain Overview, from <a href=https://python.langchain.com/v0.1/docs/get_started/introduction>Langchain Introduction</a>\n",
        "    </figcaption>\n",
        "</figure>\n",
        "\n",
        "### Quick Start\n",
        "To, as it says - start quickly - get started using the [Quick Start](https://python.langchain.com/v0.1/docs/get_started/quickstart/) page.\n",
        "\n",
        "### Details of Individual Composition Components\n",
        "To learn more about any of the individual components used below, use the [Components Page](https://python.langchain.com/v0.1/docs/modules/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "477bff1b",
      "metadata": {
        "id": "477bff1b"
      },
      "source": [
        "## Review of python formatted strings\n",
        "To prepare ourselves for langchain, we'll first review formatted strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cff2bcd",
      "metadata": {
        "id": "2cff2bcd"
      },
      "outputs": [],
      "source": [
        "# basic functionality of print\n",
        "print('Tell me a story about cats')\n",
        "\n",
        "# with variables\n",
        "prompt_string = 'Tell me a story about cats'\n",
        "print('As string ', prompt_string)\n",
        "\n",
        "# as formatted string\n",
        "prompt_string = 'Tell me a story about cats'\n",
        "print(f\"With formatted string: {prompt_string}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d708c91c",
      "metadata": {
        "id": "d708c91c"
      },
      "source": [
        "Motivating example: you are building a GPT that tells stories. The user just needs to provide the topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afd22772",
      "metadata": {
        "id": "afd22772"
      },
      "outputs": [],
      "source": [
        "# as a template string\n",
        "string_prompt_template = f\"Tell me a story about {{topic}}\"\n",
        "string_prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70e55c40",
      "metadata": {
        "id": "70e55c40"
      },
      "outputs": [],
      "source": [
        "# you can fill in the template at a later time\n",
        "string_prompt_template.format(topic='cats')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a7ffaf4",
      "metadata": {
        "id": "6a7ffaf4"
      },
      "source": [
        "## Langchain Prompt Templates\n",
        "> Formatting and arranging prompt strings\n",
        "\n",
        "Langchain prompt templates work just like this, but with additional functionality targeted towards LLM interaction. There are lots of different prompt templates, but here, we'll focus on two: `PromptTemplate`, and `ChatPromptTemplate`.\n",
        "\n",
        "**Additional resources**: [Guide on Prompt Templates](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71f3192e",
      "metadata": {
        "id": "71f3192e"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef3a0cbf",
      "metadata": {
        "id": "ef3a0cbf"
      },
      "outputs": [],
      "source": [
        "# create system messsage for shorter responses\n",
        "brief_system_message = 'You are a helpful assistant. Be brief, succinct, and clear in your responses. Only answer what is asked.'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f581ae79",
      "metadata": {
        "id": "f581ae79"
      },
      "source": [
        "### PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1\n",
        "template = \"\"\"\n",
        "You are a helpful assistant. Answer the user's question based ONLY on the provided context.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "context = \"RAG stands for retrieval augmented generation\"\n",
        "question = \"What is RAG?\"\n",
        "\n",
        "template.format(context=context, question=question)"
      ],
      "metadata": {
        "id": "K78hnlwKp3bA"
      },
      "id": "K78hnlwKp3bA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lc_template = ChatPromptTemplate.from_template(template)\n",
        "flc = lc_template.invoke({'context': context, 'question':question})\n",
        "print(flc) # chat prompt template\n",
        "print(flc.messages[0]) # basemessage\n",
        "print(flc.messages[0].content) # content\n",
        "print(flc.messages[0].type) # role"
      ],
      "metadata": {
        "id": "o3v1qDaSqSol"
      },
      "id": "o3v1qDaSqSol",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dda49b6",
      "metadata": {
        "id": "4dda49b6"
      },
      "outputs": [],
      "source": [
        "# Example 2\n",
        "template_string = \"Recommend a song for someone who likes {genre} music and is feeling {mood}.\"\n",
        "template = PromptTemplate.from_template(template_string)\n",
        "istr = template.invoke({\"genre\": \"hiphop\", \"mood\": \"sad\"})\n",
        "fstr = template.format(genre='hiphop', mood='good')\n",
        "istr.text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\n",
        "You are a brilliant research assistant. Use the following context to answer the user's question.\n",
        "If the context does not contain the answer, politely state that you cannot answer based on the provided documents.\n",
        "\n",
        "Context:\n",
        "---\n",
        "{context}\n",
        "---\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a brilliant research assistant, dedicated to accurate answers.\"),\n",
        "        # Note the use of the RAG_TEMPLATE string here for the main user prompt\n",
        "        (\"human\", RAG_TEMPLATE),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Simulate the RAG inputs\n",
        "simulated_context = \"The LangChain Expression Language (LCEL) uses the pipe '|' operator to chain runnables.\"\n",
        "simulated_question = \"How do you chain components in LCEL?\"\n",
        "\n",
        "# Format the RAG prompt\n",
        "rag_messages = rag_prompt.invoke({\n",
        "    \"context\": simulated_context,\n",
        "    \"question\": simulated_question\n",
        "})\n",
        "\n",
        "print(\"--- RAG-ready Message Content ---\")\n",
        "print(rag_messages.messages[1].content)"
      ],
      "metadata": {
        "id": "chOSaMy8wp0C"
      },
      "id": "chOSaMy8wp0C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "38033eec",
      "metadata": {
        "id": "38033eec"
      },
      "source": [
        "### ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "f253a330",
      "metadata": {
        "id": "f253a330",
        "outputId": "037a35fc-a82e-4bf0-ac8d-482dfcfbe5b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='tell me a story about cats', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# create prompt template\n",
        "lc_chat_prompt_template = ChatPromptTemplate.from_template(\"tell me a story about {topic}\")\n",
        "\n",
        "# has invocation functionality resulting to chat-style messages\n",
        "lc_chat_prompt_template.invoke({'topic':'cats'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6896b8a",
      "metadata": {
        "id": "f6896b8a"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "# create message-based chat prompt template\n",
        "lc_chat_prompt_template = ChatPromptTemplate.from_messages(\n",
        "    # Use tuples (role, template_string) for messages with placeholders\n",
        "    [\n",
        "        # System message (static content, no need for formatting)\n",
        "        (\"system\", \"You are a helpful and witty assistant.\"),\n",
        "\n",
        "        # Human message (uses placeholder {topic})\n",
        "        (\"human\", \"Tell me a fun fact about {topic}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# invoke the chat prompt template\n",
        "formatted_prompt = lc_chat_prompt_template.invoke({'topic':'cats'})\n",
        "\n",
        "print(\"--- Resulting Prompt Messages ---\")\n",
        "for message in formatted_prompt.messages:\n",
        "    print(f\"Role: {message.type.capitalize()}\")\n",
        "    print(f\"Content: {message.content}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# --- 2. Define the Chat History (Previous Turns) ---\n",
        "\n",
        "# This is a list of previous interactions. Note that it uses\n",
        "# HumanMessage and AIMessage objects (or the tuple shorthand)\n",
        "# to define the role and content.\n",
        "conversation_history = [\n",
        "    HumanMessage(content=\"Hello, I am planning a trip to Italy.\"),\n",
        "    AIMessage(content=\"That sounds wonderful! Italy has many great cities. Which city interests you the most?\"),\n",
        "]\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "        # 1. System Instruction: Sets the persona/rules\n",
        "        (\"system\", \"You are a helpful travel agent specialized in European cities. Keep your answers brief and friendly.\"),\n",
        "\n",
        "        # 2. History Placeholder: This is where ALL previous messages will be injected\n",
        "        # The variable name 'chat_history' must match the key used in the .invoke() call\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "\n",
        "        # 3. Human's Current Question: This is the newest question from the user\n",
        "        # It uses a standard input variable {input}\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "messages = prompt_template.invoke({'chat_history': conversation_history, 'input': 'What is the best way to travel from Rome to Florence?'})\n",
        "messages"
      ],
      "metadata": {
        "id": "KAHsEBhI0ufC",
        "outputId": "34f6f954-f13b-4772-83c5-83f71d728251",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KAHsEBhI0ufC",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful travel agent specialized in European cities. Keep your answers brief and friendly.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, I am planning a trip to Italy.', additional_kwargs={}, response_metadata={}), AIMessage(content='That sounds wonderful! Italy has many great cities. Which city interests you the most?', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the best way to travel from Rome to Florence?', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20703088",
      "metadata": {
        "id": "20703088"
      },
      "source": [
        "## Langchain Expression Language (LCEL)\n",
        "**Resource:** [LCEL Overview](https://python.langchain.com/v0.1/docs/expression_language/)\n",
        "Main Points:\n",
        "* Runnable Protocol\n",
        "* Known inputs and outputs on invoke\n",
        "* Flexibility in chain assembly\n",
        "* [Standard Interface](https://python.langchain.com/v0.1/docs/expression_language/interface/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2D0GyPPpedXw",
      "metadata": {
        "id": "2D0GyPPpedXw"
      },
      "source": [
        "# Basic Model Chains/ Model I/O\n",
        "\n",
        "**Resource**: [Detailed Guide](https://python.langchain.com/v0.1/docs/modules/)\n",
        "\n",
        "## Basic Prompt/Model Chain\n",
        "See [Prompt+LLM](https://python.langchain.com/docs/expression_language/cookbook/prompt_llm_parser) for more information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a5d1b35f",
      "metadata": {
        "id": "a5d1b35f"
      },
      "outputs": [],
      "source": [
        "# from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "4f068a62-9a56-420a-be6f-2bf3bb805d04",
      "metadata": {
        "id": "4f068a62-9a56-420a-be6f-2bf3bb805d04"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"tell me a oneliner joke about {topic}\")\n",
        "# model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "# chain = prompt | model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "1GVTTau0fsdd",
      "metadata": {
        "id": "1GVTTau0fsdd",
        "outputId": "690df91c-0153-4b7e-f7f0-c7b8ea344b8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='tell me a oneliner joke about cats', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='I\\'m pretty sure my cat thinks my job is \"human can opener.\"', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--77e45cb6-9ac8-481b-9ed9-14766ad8e143-0', usage_metadata={'input_tokens': 10, 'output_tokens': 1637, 'total_tokens': 1647, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1621}})]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# Observe what the prompt looks like when we substitute words into it\n",
        "chat_prompt_value= prompt.invoke({'topic':\"cats\"})\n",
        "ai_message = model.invoke(chat_prompt)\n",
        "messages = chat_prompt_value.messages\n",
        "messages.append(ai_message)\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "922XjNNiAPL_"
      },
      "id": "922XjNNiAPL_",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "chain = prompt | model\n"
      ],
      "metadata": {
        "id": "0bHfsd_9MT6j"
      },
      "id": "0bHfsd_9MT6j",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "VP7khR3zd2OJ",
      "metadata": {
        "id": "VP7khR3zd2OJ",
        "outputId": "01c41303-d148-4890-af1d-9313a6c21114",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"Cats don't have owners, they have live-in staff.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--5494e93d-e404-4097-9f8f-b5e4f8a6804b-0' usage_metadata={'input_tokens': 10, 'output_tokens': 1163, 'total_tokens': 1173, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1149}}\n"
          ]
        }
      ],
      "source": [
        "# Now, actually call the entire chain on it\n",
        "res = chain.invoke({'topic': 'cats'})\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d753e10d",
      "metadata": {
        "id": "d753e10d"
      },
      "source": [
        "A little helper visualization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "53dc608a",
      "metadata": {
        "id": "53dc608a",
        "outputId": "800cfe20-e4f9-445f-b759-5c2a86dbf147",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        +-------------+          \n",
            "        | PromptInput |          \n",
            "        +-------------+          \n",
            "                *                \n",
            "                *                \n",
            "                *                \n",
            "     +--------------------+      \n",
            "     | ChatPromptTemplate |      \n",
            "     +--------------------+      \n",
            "                *                \n",
            "                *                \n",
            "                *                \n",
            "   +------------------------+    \n",
            "   | ChatGoogleGenerativeAI |    \n",
            "   +------------------------+    \n",
            "                *                \n",
            "                *                \n",
            "                *                \n",
            "+------------------------------+ \n",
            "| ChatGoogleGenerativeAIOutput | \n",
            "+------------------------------+ \n"
          ]
        }
      ],
      "source": [
        "# create visualization\n",
        "chain.get_graph().print_ascii()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LUCuzx2HfeVx",
      "metadata": {
        "id": "LUCuzx2HfeVx"
      },
      "source": [
        "## Even more simplified prompt chains"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "b06rjHV_Nebj"
      },
      "id": "b06rjHV_Nebj",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "eM3wDIpnfg3f",
      "metadata": {
        "id": "eM3wDIpnfg3f"
      },
      "outputs": [],
      "source": [
        "# Create total user prompt chain\n",
        "prompt = ChatPromptTemplate.from_template(\"{text}\")\n",
        "\n",
        "# Add output parser\n",
        "chain = prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8hStGS1kfonX",
      "metadata": {
        "id": "8hStGS1kfonX",
        "outputId": "555cb48e-6597-486f-f907-607aaf48272f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Raga is a melodic framework in Indian classical music, defined by specific notes, characteristic phrases, and rules for their use, designed to evoke a particular mood or emotion.\n"
          ]
        }
      ],
      "source": [
        "# Now, the user can submit literally whatever\n",
        "res = chain.invoke({'text':\"What is Rag in a single line\"})\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "958be68e",
      "metadata": {
        "id": "958be68e"
      },
      "source": [
        "## What just happened? Inspecting model behavior\n",
        "Several ways to do this:\n",
        "* `langchain` verbosity/debugging\n",
        "* `langsmith`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db9684a3",
      "metadata": {
        "id": "db9684a3"
      },
      "source": [
        "### Langchain\n",
        "Resource: [Guides -> Langchain Debugging](https://python.langchain.com/v0.1/docs/guides/development/debugging/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e76bf06e",
      "metadata": {
        "id": "e76bf06e"
      },
      "outputs": [],
      "source": [
        "from langchain.globals import set_debug, set_verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "88edac26",
      "metadata": {
        "id": "88edac26"
      },
      "outputs": [],
      "source": [
        "set_debug(True)\n",
        "set_verbose(True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"{text}\")\n",
        "# alright. When there is only one place holder we don't need to specify it explicitely when invoking the chain\n",
        "prompt.invoke(\"what is it?\")"
      ],
      "metadata": {
        "id": "yKCOJf-iXH8t",
        "outputId": "74f89711-4cc3-455f-d751-3786a1c2e68a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yKCOJf-iXH8t",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='what is it?', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1984a1a0",
      "metadata": {
        "id": "1984a1a0",
        "outputId": "ec53adb5-da6b-48af-eb30-ae39991301bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"What is a python f-string?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"What is a python f-string?\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: What is a python f-string?\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatGoogleGenerativeAI] [27.45s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"A Python **f-string** (short for \\\"formatted string literal\\\") is a powerful and convenient way to embed expressions inside string literals. Introduced in Python 3.6, it provides a concise and readable syntax for creating formatted strings.\\n\\n### How to Create an f-string\\n\\nYou create an f-string by prefixing a string literal with the letter `f` or `F` (e.g., `f\\\"...\\\"` or `F\\\"...\\\"`). Inside the string, you can then place Python expressions within curly braces `{}`. These expressions will be evaluated at runtime and their results inserted into the string.\\n\\n### Key Features and Benefits\\n\\n1.  **Conciseness and Readability:**\\n    *   They are much more readable than older methods like the `%` operator or `str.format()` because the variables and expressions are placed directly inline with the string text.\\n\\n    ```python\\n    name = \\\"Alice\\\"\\n    age = 30\\n\\n    # f-string\\n    print(f\\\"My name is {name} and I am {age} years old.\\\")\\n    # Output: My name is Alice and I am 30 years old.\\n\\n    # Equivalent using str.format() - more verbose\\n    print(\\\"My name is {} and I am {} years old.\\\".format(name, age))\\n\\n    # Equivalent using % operator - older, less readable\\n    print(\\\"My name is %s and I am %d years old.\\\" % (name, age))\\n    ```\\n\\n2.  **Embed Expressions Directly:**\\n    *   You're not limited to just variables; you can put any valid Python expression inside the curly braces.\\n\\n    ```python\\n    price = 19.99\\n    quantity = 3\\n    total = price * quantity\\n\\n    print(f\\\"The total for {quantity} items at ${price:.2f} each is ${total:.2f}.\\\")\\n    # Output: The total for 3 items at $19.99 each is $59.97.\\n\\n    # Function calls\\n    def greet(person):\\n        return f\\\"Hello, {person.upper()}!\\\"\\n    print(greet(\\\"bob\\\"))\\n    # Output: Hello, BOB!\\n\\n    # Method calls\\n    s = \\\"python\\\"\\n    print(f\\\"The string in uppercase is {s.upper()}.\\\")\\n    # Output: The string in uppercase is PYTHON.\\n\\n    # Conditional expressions (ternary operator)\\n    num = 10\\n    print(f\\\"The number {num} is {'even' if num % 2 == 0 else 'odd'}.\\\")\\n    # Output: The number 10 is even.\\n    ```\\n\\n3.  **Formatting Mini-Language:**\\n    *   You can apply formatting specifiers (similar to `str.format()`'s mini-language) after a colon `:` within the curly braces to control alignment, precision, padding, number formatting, and more.\\n\\n    ```python\\n    pi = 3.14159265\\n    print(f\\\"Pi to 2 decimal places: {pi:.2f}\\\")  # Precision for floats\\n    # Output: Pi to 2 decimal places: 3.14\\n\\n    value = 1234567\\n    print(f\\\"Large number with comma separator: {value:,}\\\")\\n    # Output: Large number with comma separator: 1,234,567\\n\\n    percentage = 0.75\\n    print(f\\\"Percentage: {percentage:.1%}\\\") # Percentage with one decimal\\n    # Output: Percentage: 75.0%\\n\\n    text = \\\"hello\\\"\\n    print(f\\\"Centered text: {text:^15}\\\") # Centered in a field of 15 characters\\n    # Output: Centered text:      hello\\n\\n    import datetime\\n    now = datetime.datetime.now()\\n    print(f\\\"Current date and time: {now:%Y-%m-%d %H:%M:%S}\\\")\\n    # Output: Current date and time: 2023-10-27 10:30:45 (example)\\n    ```\\n\\n4.  **Debugging (`=` Specifier - Python 3.8+):**\\n    *   A very handy feature for debugging is the `=` specifier, which prints the expression followed by an equals sign and its evaluated value.\\n\\n    ```python\\n    x = 10\\n    y = 20\\n    print(f\\\"{x=}, {y=}, {x*y=}\\\")\\n    # Output: x=10, y=20, x*y=200\\n    ```\\n\\n5.  **Type Conversions (`!s`, `!r`, `!a`):**\\n    *   You can specify how an expression should be converted to a string using `!s` (str), `!r` (repr), or `!a` (ascii).\\n\\n    ```python\\n    my_list = [1, 2, 3]\\n    print(f\\\"String representation: {my_list!s}\\\")\\n    # Output: String representation: [1, 2, 3]\\n    print(f\\\"Raw representation: {my_list!r}\\\")\\n    # Output: Raw representation: [1, 2, 3] (often same for simple types, but useful for custom objects)\\n\\n    class MyClass:\\n        def __repr__(self):\\n            return \\\"MyClass(repr)\\\"\\n        def __str__(self):\\n            return \\\"MyClass(str)\\\"\\n\\n    obj = MyClass()\\n    print(f\\\"{obj!s}\\\") # Uses __str__\\n    # Output: MyClass(str)\\n    print(f\\\"{obj!r}\\\") # Uses __repr__\\n    # Output: MyClass(repr)\\n    ```\\n\\n### Important Considerations:\\n\\n*   **Python Version:** F-strings require Python 3.6 or later.\\n*   **Quoting:** Be careful with quotes inside f-strings. If your f-string uses double quotes, use single quotes inside for strings, and vice-versa.\\n    ```python\\n    message = \\\"Hello\\\"\\n    print(f'He said \\\"{message}\\\" today.')\\n    # Output: He said \\\"Hello\\\" today.\\n    ```\\n*   **Multiline f-strings:** You can use triple quotes for multiline f-strings, just like regular strings.\\n    ```python\\n    item = \\\"Laptop\\\"\\n    price = 1200\\n    description = f\\\"\\\"\\\"\\n    Product: {item}\\n    Price: ${price:.2f}\\n    Status: In Stock\\n    \\\"\\\"\\\"\\n    print(description)\\n    ```\\n\\nIn summary, f-strings are the recommended and most modern way to format strings in Python due to their simplicity, power, and readability.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"model_name\": \"gemini-2.5-flash\",\n",
            "          \"safety_ratings\": []\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"A Python **f-string** (short for \\\"formatted string literal\\\") is a powerful and convenient way to embed expressions inside string literals. Introduced in Python 3.6, it provides a concise and readable syntax for creating formatted strings.\\n\\n### How to Create an f-string\\n\\nYou create an f-string by prefixing a string literal with the letter `f` or `F` (e.g., `f\\\"...\\\"` or `F\\\"...\\\"`). Inside the string, you can then place Python expressions within curly braces `{}`. These expressions will be evaluated at runtime and their results inserted into the string.\\n\\n### Key Features and Benefits\\n\\n1.  **Conciseness and Readability:**\\n    *   They are much more readable than older methods like the `%` operator or `str.format()` because the variables and expressions are placed directly inline with the string text.\\n\\n    ```python\\n    name = \\\"Alice\\\"\\n    age = 30\\n\\n    # f-string\\n    print(f\\\"My name is {name} and I am {age} years old.\\\")\\n    # Output: My name is Alice and I am 30 years old.\\n\\n    # Equivalent using str.format() - more verbose\\n    print(\\\"My name is {} and I am {} years old.\\\".format(name, age))\\n\\n    # Equivalent using % operator - older, less readable\\n    print(\\\"My name is %s and I am %d years old.\\\" % (name, age))\\n    ```\\n\\n2.  **Embed Expressions Directly:**\\n    *   You're not limited to just variables; you can put any valid Python expression inside the curly braces.\\n\\n    ```python\\n    price = 19.99\\n    quantity = 3\\n    total = price * quantity\\n\\n    print(f\\\"The total for {quantity} items at ${price:.2f} each is ${total:.2f}.\\\")\\n    # Output: The total for 3 items at $19.99 each is $59.97.\\n\\n    # Function calls\\n    def greet(person):\\n        return f\\\"Hello, {person.upper()}!\\\"\\n    print(greet(\\\"bob\\\"))\\n    # Output: Hello, BOB!\\n\\n    # Method calls\\n    s = \\\"python\\\"\\n    print(f\\\"The string in uppercase is {s.upper()}.\\\")\\n    # Output: The string in uppercase is PYTHON.\\n\\n    # Conditional expressions (ternary operator)\\n    num = 10\\n    print(f\\\"The number {num} is {'even' if num % 2 == 0 else 'odd'}.\\\")\\n    # Output: The number 10 is even.\\n    ```\\n\\n3.  **Formatting Mini-Language:**\\n    *   You can apply formatting specifiers (similar to `str.format()`'s mini-language) after a colon `:` within the curly braces to control alignment, precision, padding, number formatting, and more.\\n\\n    ```python\\n    pi = 3.14159265\\n    print(f\\\"Pi to 2 decimal places: {pi:.2f}\\\")  # Precision for floats\\n    # Output: Pi to 2 decimal places: 3.14\\n\\n    value = 1234567\\n    print(f\\\"Large number with comma separator: {value:,}\\\")\\n    # Output: Large number with comma separator: 1,234,567\\n\\n    percentage = 0.75\\n    print(f\\\"Percentage: {percentage:.1%}\\\") # Percentage with one decimal\\n    # Output: Percentage: 75.0%\\n\\n    text = \\\"hello\\\"\\n    print(f\\\"Centered text: {text:^15}\\\") # Centered in a field of 15 characters\\n    # Output: Centered text:      hello\\n\\n    import datetime\\n    now = datetime.datetime.now()\\n    print(f\\\"Current date and time: {now:%Y-%m-%d %H:%M:%S}\\\")\\n    # Output: Current date and time: 2023-10-27 10:30:45 (example)\\n    ```\\n\\n4.  **Debugging (`=` Specifier - Python 3.8+):**\\n    *   A very handy feature for debugging is the `=` specifier, which prints the expression followed by an equals sign and its evaluated value.\\n\\n    ```python\\n    x = 10\\n    y = 20\\n    print(f\\\"{x=}, {y=}, {x*y=}\\\")\\n    # Output: x=10, y=20, x*y=200\\n    ```\\n\\n5.  **Type Conversions (`!s`, `!r`, `!a`):**\\n    *   You can specify how an expression should be converted to a string using `!s` (str), `!r` (repr), or `!a` (ascii).\\n\\n    ```python\\n    my_list = [1, 2, 3]\\n    print(f\\\"String representation: {my_list!s}\\\")\\n    # Output: String representation: [1, 2, 3]\\n    print(f\\\"Raw representation: {my_list!r}\\\")\\n    # Output: Raw representation: [1, 2, 3] (often same for simple types, but useful for custom objects)\\n\\n    class MyClass:\\n        def __repr__(self):\\n            return \\\"MyClass(repr)\\\"\\n        def __str__(self):\\n            return \\\"MyClass(str)\\\"\\n\\n    obj = MyClass()\\n    print(f\\\"{obj!s}\\\") # Uses __str__\\n    # Output: MyClass(str)\\n    print(f\\\"{obj!r}\\\") # Uses __repr__\\n    # Output: MyClass(repr)\\n    ```\\n\\n### Important Considerations:\\n\\n*   **Python Version:** F-strings require Python 3.6 or later.\\n*   **Quoting:** Be careful with quotes inside f-strings. If your f-string uses double quotes, use single quotes inside for strings, and vice-versa.\\n    ```python\\n    message = \\\"Hello\\\"\\n    print(f'He said \\\"{message}\\\" today.')\\n    # Output: He said \\\"Hello\\\" today.\\n    ```\\n*   **Multiline f-strings:** You can use triple quotes for multiline f-strings, just like regular strings.\\n    ```python\\n    item = \\\"Laptop\\\"\\n    price = 1200\\n    description = f\\\"\\\"\\\"\\n    Product: {item}\\n    Price: ${price:.2f}\\n    Status: In Stock\\n    \\\"\\\"\\\"\\n    print(description)\\n    ```\\n\\nIn summary, f-strings are the recommended and most modern way to format strings in Python due to their simplicity, power, and readability.\",\n",
            "            \"response_metadata\": {\n",
            "              \"prompt_feedback\": {\n",
            "                \"block_reason\": 0,\n",
            "                \"safety_ratings\": []\n",
            "              },\n",
            "              \"finish_reason\": \"STOP\",\n",
            "              \"model_name\": \"gemini-2.5-flash\",\n",
            "              \"safety_ratings\": []\n",
            "            },\n",
            "            \"type\": \"ai\",\n",
            "            \"id\": \"run--aa7c1ca8-ab1c-47d4-9d4e-e6665a3b896f-0\",\n",
            "            \"usage_metadata\": {\n",
            "              \"input_tokens\": 9,\n",
            "              \"output_tokens\": 2737,\n",
            "              \"total_tokens\": 2746,\n",
            "              \"input_token_details\": {\n",
            "                \"cache_read\": 0\n",
            "              },\n",
            "              \"output_token_details\": {\n",
            "                \"reasoning\": 1248\n",
            "              }\n",
            "            },\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"prompt_feedback\": {\n",
            "      \"block_reason\": 0,\n",
            "      \"safety_ratings\": []\n",
            "    }\n",
            "  },\n",
            "  \"run\": null,\n",
            "  \"type\": \"LLMResult\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"A Python **f-string** (short for \\\"formatted string literal\\\") is a powerful and convenient way to embed expressions inside string literals. Introduced in Python 3.6, it provides a concise and readable syntax for creating formatted strings.\\n\\n### How to Create an f-string\\n\\nYou create an f-string by prefixing a string literal with the letter `f` or `F` (e.g., `f\\\"...\\\"` or `F\\\"...\\\"`). Inside the string, you can then place Python expressions within curly braces `{}`. These expressions will be evaluated at runtime and their results inserted into the string.\\n\\n### Key Features and Benefits\\n\\n1.  **Conciseness and Readability:**\\n    *   They are much more readable than older methods like the `%` operator or `str.format()` because the variables and expressions are placed directly inline with the string text.\\n\\n    ```python\\n    name = \\\"Alice\\\"\\n    age = 30\\n\\n    # f-string\\n    print(f\\\"My name is {name} and I am {age} years old.\\\")\\n    # Output: My name is Alice and I am 30 years old.\\n\\n    # Equivalent using str.format() - more verbose\\n    print(\\\"My name is {} and I am {} years old.\\\".format(name, age))\\n\\n    # Equivalent using % operator - older, less readable\\n    print(\\\"My name is %s and I am %d years old.\\\" % (name, age))\\n    ```\\n\\n2.  **Embed Expressions Directly:**\\n    *   You're not limited to just variables; you can put any valid Python expression inside the curly braces.\\n\\n    ```python\\n    price = 19.99\\n    quantity = 3\\n    total = price * quantity\\n\\n    print(f\\\"The total for {quantity} items at ${price:.2f} each is ${total:.2f}.\\\")\\n    # Output: The total for 3 items at $19.99 each is $59.97.\\n\\n    # Function calls\\n    def greet(person):\\n        return f\\\"Hello, {person.upper()}!\\\"\\n    print(greet(\\\"bob\\\"))\\n    # Output: Hello, BOB!\\n\\n    # Method calls\\n    s = \\\"python\\\"\\n    print(f\\\"The string in uppercase is {s.upper()}.\\\")\\n    # Output: The string in uppercase is PYTHON.\\n\\n    # Conditional expressions (ternary operator)\\n    num = 10\\n    print(f\\\"The number {num} is {'even' if num % 2 == 0 else 'odd'}.\\\")\\n    # Output: The number 10 is even.\\n    ```\\n\\n3.  **Formatting Mini-Language:**\\n    *   You can apply formatting specifiers (similar to `str.format()`'s mini-language) after a colon `:` within the curly braces to control alignment, precision, padding, number formatting, and more.\\n\\n    ```python\\n    pi = 3.14159265\\n    print(f\\\"Pi to 2 decimal places: {pi:.2f}\\\")  # Precision for floats\\n    # Output: Pi to 2 decimal places: 3.14\\n\\n    value = 1234567\\n    print(f\\\"Large number with comma separator: {value:,}\\\")\\n    # Output: Large number with comma separator: 1,234,567\\n\\n    percentage = 0.75\\n    print(f\\\"Percentage: {percentage:.1%}\\\") # Percentage with one decimal\\n    # Output: Percentage: 75.0%\\n\\n    text = \\\"hello\\\"\\n    print(f\\\"Centered text: {text:^15}\\\") # Centered in a field of 15 characters\\n    # Output: Centered text:      hello\\n\\n    import datetime\\n    now = datetime.datetime.now()\\n    print(f\\\"Current date and time: {now:%Y-%m-%d %H:%M:%S}\\\")\\n    # Output: Current date and time: 2023-10-27 10:30:45 (example)\\n    ```\\n\\n4.  **Debugging (`=` Specifier - Python 3.8+):**\\n    *   A very handy feature for debugging is the `=` specifier, which prints the expression followed by an equals sign and its evaluated value.\\n\\n    ```python\\n    x = 10\\n    y = 20\\n    print(f\\\"{x=}, {y=}, {x*y=}\\\")\\n    # Output: x=10, y=20, x*y=200\\n    ```\\n\\n5.  **Type Conversions (`!s`, `!r`, `!a`):**\\n    *   You can specify how an expression should be converted to a string using `!s` (str), `!r` (repr), or `!a` (ascii).\\n\\n    ```python\\n    my_list = [1, 2, 3]\\n    print(f\\\"String representation: {my_list!s}\\\")\\n    # Output: String representation: [1, 2, 3]\\n    print(f\\\"Raw representation: {my_list!r}\\\")\\n    # Output: Raw representation: [1, 2, 3] (often same for simple types, but useful for custom objects)\\n\\n    class MyClass:\\n        def __repr__(self):\\n            return \\\"MyClass(repr)\\\"\\n        def __str__(self):\\n            return \\\"MyClass(str)\\\"\\n\\n    obj = MyClass()\\n    print(f\\\"{obj!s}\\\") # Uses __str__\\n    # Output: MyClass(str)\\n    print(f\\\"{obj!r}\\\") # Uses __repr__\\n    # Output: MyClass(repr)\\n    ```\\n\\n### Important Considerations:\\n\\n*   **Python Version:** F-strings require Python 3.6 or later.\\n*   **Quoting:** Be careful with quotes inside f-strings. If your f-string uses double quotes, use single quotes inside for strings, and vice-versa.\\n    ```python\\n    message = \\\"Hello\\\"\\n    print(f'He said \\\"{message}\\\" today.')\\n    # Output: He said \\\"Hello\\\" today.\\n    ```\\n*   **Multiline f-strings:** You can use triple quotes for multiline f-strings, just like regular strings.\\n    ```python\\n    item = \\\"Laptop\\\"\\n    price = 1200\\n    description = f\\\"\\\"\\\"\\n    Product: {item}\\n    Price: ${price:.2f}\\n    Status: In Stock\\n    \\\"\\\"\\\"\\n    print(description)\\n    ```\\n\\nIn summary, f-strings are the recommended and most modern way to format strings in Python due to their simplicity, power, and readability.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [27.45s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"A Python **f-string** (short for \\\"formatted string literal\\\") is a powerful and convenient way to embed expressions inside string literals. Introduced in Python 3.6, it provides a concise and readable syntax for creating formatted strings.\\n\\n### How to Create an f-string\\n\\nYou create an f-string by prefixing a string literal with the letter `f` or `F` (e.g., `f\\\"...\\\"` or `F\\\"...\\\"`). Inside the string, you can then place Python expressions within curly braces `{}`. These expressions will be evaluated at runtime and their results inserted into the string.\\n\\n### Key Features and Benefits\\n\\n1.  **Conciseness and Readability:**\\n    *   They are much more readable than older methods like the `%` operator or `str.format()` because the variables and expressions are placed directly inline with the string text.\\n\\n    ```python\\n    name = \\\"Alice\\\"\\n    age = 30\\n\\n    # f-string\\n    print(f\\\"My name is {name} and I am {age} years old.\\\")\\n    # Output: My name is Alice and I am 30 years old.\\n\\n    # Equivalent using str.format() - more verbose\\n    print(\\\"My name is {} and I am {} years old.\\\".format(name, age))\\n\\n    # Equivalent using % operator - older, less readable\\n    print(\\\"My name is %s and I am %d years old.\\\" % (name, age))\\n    ```\\n\\n2.  **Embed Expressions Directly:**\\n    *   You're not limited to just variables; you can put any valid Python expression inside the curly braces.\\n\\n    ```python\\n    price = 19.99\\n    quantity = 3\\n    total = price * quantity\\n\\n    print(f\\\"The total for {quantity} items at ${price:.2f} each is ${total:.2f}.\\\")\\n    # Output: The total for 3 items at $19.99 each is $59.97.\\n\\n    # Function calls\\n    def greet(person):\\n        return f\\\"Hello, {person.upper()}!\\\"\\n    print(greet(\\\"bob\\\"))\\n    # Output: Hello, BOB!\\n\\n    # Method calls\\n    s = \\\"python\\\"\\n    print(f\\\"The string in uppercase is {s.upper()}.\\\")\\n    # Output: The string in uppercase is PYTHON.\\n\\n    # Conditional expressions (ternary operator)\\n    num = 10\\n    print(f\\\"The number {num} is {'even' if num % 2 == 0 else 'odd'}.\\\")\\n    # Output: The number 10 is even.\\n    ```\\n\\n3.  **Formatting Mini-Language:**\\n    *   You can apply formatting specifiers (similar to `str.format()`'s mini-language) after a colon `:` within the curly braces to control alignment, precision, padding, number formatting, and more.\\n\\n    ```python\\n    pi = 3.14159265\\n    print(f\\\"Pi to 2 decimal places: {pi:.2f}\\\")  # Precision for floats\\n    # Output: Pi to 2 decimal places: 3.14\\n\\n    value = 1234567\\n    print(f\\\"Large number with comma separator: {value:,}\\\")\\n    # Output: Large number with comma separator: 1,234,567\\n\\n    percentage = 0.75\\n    print(f\\\"Percentage: {percentage:.1%}\\\") # Percentage with one decimal\\n    # Output: Percentage: 75.0%\\n\\n    text = \\\"hello\\\"\\n    print(f\\\"Centered text: {text:^15}\\\") # Centered in a field of 15 characters\\n    # Output: Centered text:      hello\\n\\n    import datetime\\n    now = datetime.datetime.now()\\n    print(f\\\"Current date and time: {now:%Y-%m-%d %H:%M:%S}\\\")\\n    # Output: Current date and time: 2023-10-27 10:30:45 (example)\\n    ```\\n\\n4.  **Debugging (`=` Specifier - Python 3.8+):**\\n    *   A very handy feature for debugging is the `=` specifier, which prints the expression followed by an equals sign and its evaluated value.\\n\\n    ```python\\n    x = 10\\n    y = 20\\n    print(f\\\"{x=}, {y=}, {x*y=}\\\")\\n    # Output: x=10, y=20, x*y=200\\n    ```\\n\\n5.  **Type Conversions (`!s`, `!r`, `!a`):**\\n    *   You can specify how an expression should be converted to a string using `!s` (str), `!r` (repr), or `!a` (ascii).\\n\\n    ```python\\n    my_list = [1, 2, 3]\\n    print(f\\\"String representation: {my_list!s}\\\")\\n    # Output: String representation: [1, 2, 3]\\n    print(f\\\"Raw representation: {my_list!r}\\\")\\n    # Output: Raw representation: [1, 2, 3] (often same for simple types, but useful for custom objects)\\n\\n    class MyClass:\\n        def __repr__(self):\\n            return \\\"MyClass(repr)\\\"\\n        def __str__(self):\\n            return \\\"MyClass(str)\\\"\\n\\n    obj = MyClass()\\n    print(f\\\"{obj!s}\\\") # Uses __str__\\n    # Output: MyClass(str)\\n    print(f\\\"{obj!r}\\\") # Uses __repr__\\n    # Output: MyClass(repr)\\n    ```\\n\\n### Important Considerations:\\n\\n*   **Python Version:** F-strings require Python 3.6 or later.\\n*   **Quoting:** Be careful with quotes inside f-strings. If your f-string uses double quotes, use single quotes inside for strings, and vice-versa.\\n    ```python\\n    message = \\\"Hello\\\"\\n    print(f'He said \\\"{message}\\\" today.')\\n    # Output: He said \\\"Hello\\\" today.\\n    ```\\n*   **Multiline f-strings:** You can use triple quotes for multiline f-strings, just like regular strings.\\n    ```python\\n    item = \\\"Laptop\\\"\\n    price = 1200\\n    description = f\\\"\\\"\\\"\\n    Product: {item}\\n    Price: ${price:.2f}\\n    Status: In Stock\\n    \\\"\\\"\\\"\\n    print(description)\\n    ```\\n\\nIn summary, f-strings are the recommended and most modern way to format strings in Python due to their simplicity, power, and readability.\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A Python **f-string** (short for \"formatted string literal\") is a powerful and convenient way to embed expressions inside string literals. Introduced in Python 3.6, it provides a concise and readable syntax for creating formatted strings.\\n\\n### How to Create an f-string\\n\\nYou create an f-string by prefixing a string literal with the letter `f` or `F` (e.g., `f\"...\"` or `F\"...\"`). Inside the string, you can then place Python expressions within curly braces `{}`. These expressions will be evaluated at runtime and their results inserted into the string.\\n\\n### Key Features and Benefits\\n\\n1.  **Conciseness and Readability:**\\n    *   They are much more readable than older methods like the `%` operator or `str.format()` because the variables and expressions are placed directly inline with the string text.\\n\\n    ```python\\n    name = \"Alice\"\\n    age = 30\\n\\n    # f-string\\n    print(f\"My name is {name} and I am {age} years old.\")\\n    # Output: My name is Alice and I am 30 years old.\\n\\n    # Equivalent using str.format() - more verbose\\n    print(\"My name is {} and I am {} years old.\".format(name, age))\\n\\n    # Equivalent using % operator - older, less readable\\n    print(\"My name is %s and I am %d years old.\" % (name, age))\\n    ```\\n\\n2.  **Embed Expressions Directly:**\\n    *   You\\'re not limited to just variables; you can put any valid Python expression inside the curly braces.\\n\\n    ```python\\n    price = 19.99\\n    quantity = 3\\n    total = price * quantity\\n\\n    print(f\"The total for {quantity} items at ${price:.2f} each is ${total:.2f}.\")\\n    # Output: The total for 3 items at $19.99 each is $59.97.\\n\\n    # Function calls\\n    def greet(person):\\n        return f\"Hello, {person.upper()}!\"\\n    print(greet(\"bob\"))\\n    # Output: Hello, BOB!\\n\\n    # Method calls\\n    s = \"python\"\\n    print(f\"The string in uppercase is {s.upper()}.\")\\n    # Output: The string in uppercase is PYTHON.\\n\\n    # Conditional expressions (ternary operator)\\n    num = 10\\n    print(f\"The number {num} is {\\'even\\' if num % 2 == 0 else \\'odd\\'}.\")\\n    # Output: The number 10 is even.\\n    ```\\n\\n3.  **Formatting Mini-Language:**\\n    *   You can apply formatting specifiers (similar to `str.format()`\\'s mini-language) after a colon `:` within the curly braces to control alignment, precision, padding, number formatting, and more.\\n\\n    ```python\\n    pi = 3.14159265\\n    print(f\"Pi to 2 decimal places: {pi:.2f}\")  # Precision for floats\\n    # Output: Pi to 2 decimal places: 3.14\\n\\n    value = 1234567\\n    print(f\"Large number with comma separator: {value:,}\")\\n    # Output: Large number with comma separator: 1,234,567\\n\\n    percentage = 0.75\\n    print(f\"Percentage: {percentage:.1%}\") # Percentage with one decimal\\n    # Output: Percentage: 75.0%\\n\\n    text = \"hello\"\\n    print(f\"Centered text: {text:^15}\") # Centered in a field of 15 characters\\n    # Output: Centered text:      hello\\n\\n    import datetime\\n    now = datetime.datetime.now()\\n    print(f\"Current date and time: {now:%Y-%m-%d %H:%M:%S}\")\\n    # Output: Current date and time: 2023-10-27 10:30:45 (example)\\n    ```\\n\\n4.  **Debugging (`=` Specifier - Python 3.8+):**\\n    *   A very handy feature for debugging is the `=` specifier, which prints the expression followed by an equals sign and its evaluated value.\\n\\n    ```python\\n    x = 10\\n    y = 20\\n    print(f\"{x=}, {y=}, {x*y=}\")\\n    # Output: x=10, y=20, x*y=200\\n    ```\\n\\n5.  **Type Conversions (`!s`, `!r`, `!a`):**\\n    *   You can specify how an expression should be converted to a string using `!s` (str), `!r` (repr), or `!a` (ascii).\\n\\n    ```python\\n    my_list = [1, 2, 3]\\n    print(f\"String representation: {my_list!s}\")\\n    # Output: String representation: [1, 2, 3]\\n    print(f\"Raw representation: {my_list!r}\")\\n    # Output: Raw representation: [1, 2, 3] (often same for simple types, but useful for custom objects)\\n\\n    class MyClass:\\n        def __repr__(self):\\n            return \"MyClass(repr)\"\\n        def __str__(self):\\n            return \"MyClass(str)\"\\n\\n    obj = MyClass()\\n    print(f\"{obj!s}\") # Uses __str__\\n    # Output: MyClass(str)\\n    print(f\"{obj!r}\") # Uses __repr__\\n    # Output: MyClass(repr)\\n    ```\\n\\n### Important Considerations:\\n\\n*   **Python Version:** F-strings require Python 3.6 or later.\\n*   **Quoting:** Be careful with quotes inside f-strings. If your f-string uses double quotes, use single quotes inside for strings, and vice-versa.\\n    ```python\\n    message = \"Hello\"\\n    print(f\\'He said \"{message}\" today.\\')\\n    # Output: He said \"Hello\" today.\\n    ```\\n*   **Multiline f-strings:** You can use triple quotes for multiline f-strings, just like regular strings.\\n    ```python\\n    item = \"Laptop\"\\n    price = 1200\\n    description = f\"\"\"\\n    Product: {item}\\n    Price: ${price:.2f}\\n    Status: In Stock\\n    \"\"\"\\n    print(description)\\n    ```\\n\\nIn summary, f-strings are the recommended and most modern way to format strings in Python due to their simplicity, power, and readability.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Basic prompt -> model -> parser chain\n",
        "chain = prompt | model | StrOutputParser()\n",
        "chain.invoke('What is a python f-string?')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e60883c",
      "metadata": {
        "id": "4e60883c"
      },
      "source": [
        "### Langsmith\n",
        "Resource: [Tracing Langchain with Langsmith](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\n",
        "\n",
        "Don't have a langsmith API Key yet? You'll need a user account on [Langsmith](https://smith.langchain.com/). Then, follow these [instructions provided by langsmith](https://docs.smith.langchain.com/#2-create-an-api-key)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6948026e",
      "metadata": {
        "id": "6948026e"
      },
      "outputs": [],
      "source": [
        "# reset this\n",
        "set_debug(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e427ad5",
      "metadata": {
        "id": "3e427ad5"
      },
      "outputs": [],
      "source": [
        "# enable tracing and set project name\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = \"false\"\n",
        "\n",
        "# uncomment the following two lines before running the cell if you have a Langchain/Langsmith API Key\n",
        "#os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
        "#os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
        "\n",
        "# set langchain project\n",
        "os.environ['LANGCHAIN_PROJECT'] = 'May15'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "9c9e76df",
      "metadata": {
        "id": "9c9e76df",
        "outputId": "90ffebb8-0250-40d6-c397-642cbd6b90cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A **Python f-string** (short for \"formatted string literal\") is a way to embed expressions inside string literals, using a minimal syntax. Introduced in **Python 3.6**, f-strings provide a concise and convenient way to create formatted strings.\\n\\n### How to use them:\\n\\nYou prefix a string literal with the letter `f` (or `F`). Inside the string, you can use curly braces `{}` to contain Python expressions. These expressions will be evaluated at runtime and their results will be inserted into the string.\\n\\n### Basic Syntax and Examples:\\n\\n```python\\nname = \"Alice\"\\nage = 30\\n\\n# Basic usage\\ngreeting = f\"Hello, my name is {name} and I am {age} years old.\"\\nprint(greeting) # Output: Hello, my name is Alice and I am 30 years old.\\n\\n# Embedding expressions\\nx = 10\\ny = 5\\ncalculation = f\"The sum of {x} and {y} is {x + y}.\"\\nprint(calculation) # Output: The sum of 10 and 5 is 15.\\n\\n# Calling functions/methods\\ndef get_status():\\n    return \"active\"\\n\\nstatus_message = f\"User status: {get_status().upper()}\"\\nprint(status_message) # Output: User status: ACTIVE\\n\\n# Using f-strings with multi-line strings (triple quotes)\\nlong_message = f\"\"\"\\nThis is a multi-line message.\\nIt includes variables like name: {name}\\nAnd a calculation: {x * y}\\n\"\"\"\\nprint(long_message)\\n# Output:\\n# This is a multi-line message.\\n# It includes variables like name: Alice\\n# And a calculation: 50\\n```\\n\\n### Key Features and Advantages:\\n\\n1.  **Conciseness and Readability:** They are much cleaner and easier to read than older string formatting methods (like the `%` operator or `str.format()`) because the variables and expressions are directly embedded where they will appear in the string.\\n2.  **Full Python Expressions:** You can put any valid Python expression inside the curly braces, not just variable names. This includes arithmetic operations, function calls, method calls, conditional expressions, etc.\\n3.  **Performance:** F-strings are generally faster than `str.format()` and the `%` operator because they are evaluated at compile time.\\n4.  **Debugging (`=` operator - Python 3.8+):** A particularly useful feature for debugging. You can add `=` after an expression inside the braces, and it will print the expression itself, an equals sign, and then its evaluated result.\\n\\n    ```python\\n    value = 42\\n    print(f\"The value is {value=}\")\\n    # Output: The value is value=42\\n\\n    price = 19.99\\n    quantity = 3\\n    print(f\"Total: {price * quantity=}\")\\n    # Output: Total: price * quantity=59.97\\n    ```\\n\\n5.  **Formatting Options (Mini-Language):** F-strings support the same format specifier mini-language as `str.format()`. This allows you to control things like:\\n    *   **Precision:** `f\"Pi: {3.14159265:.2f}\"` -> `Pi: 3.14`\\n    *   **Alignment:** `f\"Name: {name:<10}|\"` (left align, 10 chars) -> `Name: Alice     |`\\n    *   **Padding:** `f\"Number: {123:05d}\"` (pad with zeros, 5 digits) -> `Number: 00123`\\n    *   **Thousands Separator:** `f\"Big number: {123456789:,}\"` -> `Big number: 123,456,789`\\n    *   **Date/Time Formatting:** If you have a `datetime` object.\\n\\n    ```python\\n    import datetime\\n    now = datetime.datetime.now()\\n    print(f\"Current time: {now:%Y-%m-%d %H:%M:%S}\")\\n    # Output: Current time: 2023-10-27 10:30:45 (example)\\n    ```\\n\\n### Comparison to Older Methods:\\n\\n*   **`%` operator (old style):**\\n    ```python\\n    \"Hello, my name is %s and I am %d years old.\" % (name, age)\\n    ```\\n    Less readable, prone to type errors, and less flexible.\\n\\n*   **`str.format()` method:**\\n    ```python\\n    \"Hello, my name is {} and I am {} years old.\".format(name, age)\\n    \"Hello, my name is {name} and I am {age} years old.\".format(name=name, age=age)\\n    ```\\n    Better than `%`, but still separates the format string from the values, which can reduce readability for complex strings.\\n\\n### When to use f-strings:\\n\\n**Almost always!** F-strings are the recommended and most Pythonic way to format strings in modern Python (3.6+). They offer the best combination of readability, flexibility, and performance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# use a the basic chain from above\n",
        "chain = (prompt | model | StrOutputParser()) #add new component for tracing\n",
        "response = chain.invoke(\"What is a python f-string?\")\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d4399e",
      "metadata": {
        "id": "11d4399e"
      },
      "source": [
        "#### View langsmith traces\n",
        "We can take a look at this trace on [Langsmith](https://smith.langchain.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "545b8841",
      "metadata": {
        "id": "545b8841"
      },
      "source": [
        "## Adding Memory\n",
        "Adapted from: [LCEL Adding Message History](https://python.langchain.com/v0.1/docs/expression_language/how_to/message_history/)\n",
        "Also see:\n",
        "- [Langchain -> Use Cases -> Chatbots -> Memory Management](https://python.langchain.com/v0.1/docs/use_cases/chatbots/memory_management/)\n",
        "- [Components -> More -> Memory](https://python.langchain.com/v0.1/docs/modules/memory/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "0873b34a",
      "metadata": {
        "id": "0873b34a"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.prompts import MessagesPlaceholder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "9843f11f",
      "metadata": {
        "id": "9843f11f"
      },
      "outputs": [],
      "source": [
        "# create chat template with standard elements\n",
        "# model = ChatOpenAI(name='gpt-3.5-turbo')\n",
        "# 1. Define the System Instruction\n",
        "brief_system_message = (\n",
        "    \"You are a concise and helpful AI assistant. Always respond with \"\n",
        "    \"a single sentence, using the previous conversation for context.\"\n",
        ")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", brief_system_message),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{most_recent_user_message}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "turns_chain = prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = (\n",
        "    \"You are a concise and helpful AI assistant. Always reponsd with \"\n",
        "    \"a single sentencce, using the previous conversation for context.\"\n",
        ")\n",
        "ex_prompt = ChatPromptTemplate.from_messages([\n",
        "    ('system', system_message),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    ('human', \"{most_recent_user_message}\")\n",
        "])\n",
        "ex_prompt\n",
        "history = [('human', 'tell me a joke about cats'), ('ai', 'cats jump on beds')]\n",
        "user_message = 'what is so funny about that joke?'\n",
        "ex_turns_chain = ex_prompt | model | StrOutputParser()\n",
        "\n",
        "ex_turns_chain.invoke({'chat_history': history, 'most_recent_user_message': user_message})"
      ],
      "metadata": {
        "id": "ep7M7ijbPttu",
        "outputId": "203a4e50-b20c-45ea-cc8f-1da36763b4b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "ep7M7ijbPttu",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"It's funny because cats often inconveniently jump on beds, especially when you're trying to sleep or have just made the bed.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "37b39d13",
      "metadata": {
        "id": "37b39d13",
        "outputId": "7c80ed4f-4224-4bb3-eda4-3bc62e85e9f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There was no inherent humor or punchline in that factual statement.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# quickly try out chain, pretending we've already said something to the system\n",
        "first_chat_turn_messages = [(\"human\", \"Tell me a joke about cats\"),\n",
        "                            (\"ai\", \"Cats jump on beds\")]\n",
        "\n",
        "next_user_message = \"What was funny about that joke?\"\n",
        "turns_chain.invoke({'most_recent_user_message': next_user_message,\n",
        "                    'chat_history': first_chat_turn_messages})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "2f37fa72",
      "metadata": {
        "id": "2f37fa72"
      },
      "outputs": [],
      "source": [
        "# all saved conversations\n",
        "chat_conversation_threads = {}\n",
        "\n",
        "# define function to create new conversation or load old one based on session_id\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in chat_conversation_threads:\n",
        "        chat_conversation_threads[session_id] = ChatMessageHistory()\n",
        "    return chat_conversation_threads[session_id]\n",
        "\n",
        "# create chat history enabled chain\n",
        "chat_with_message_history = RunnableWithMessageHistory(\n",
        "    turns_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"most_recent_user_message\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ").with_config(run_name = 'Chat with Message History')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ac18605",
      "metadata": {
        "id": "4ac18605"
      },
      "source": [
        "Let's try it!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_conversation = {}\n",
        "def get_user_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in chat_conversation:\n",
        "        chat_conversation[session_id] = ChatMessageHistory()\n",
        "    return chat_conversation[session_id]\n",
        "\n",
        "chat_with_history = RunnableWithMessageHistory(\n",
        "    turns_chain, # chain to invoke\n",
        "    get_user_session_history, # method to call to store or retrieve the user history\n",
        "    input_messages_key=\"most_recent_user_message\", # tells which key in the invocation input contains the user messages\n",
        "    history_messages_key='chat_history', # which key in the chain expects the chat history. Matches to the MessagesPlaceHolder variable name\n",
        ").with_config(run_name = \"Chat with Message History\")"
      ],
      "metadata": {
        "id": "K7hGWqGikP56"
      },
      "id": "K7hGWqGikP56",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_message = \"tell me a joke\"\n",
        "session_id = 'user1'\n",
        "chat_with_history.invoke({'most_recent_user_message': user_message},\n",
        "                         config={\"configurable\": {\"session_id\": session_id}})"
      ],
      "metadata": {
        "id": "8UHeORErlzBL",
        "outputId": "dee936cc-5bcf-43bf-d021-be665fb030ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "8UHeORErlzBL",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why don don't scientists trust atoms? Because they make up everything!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "076cfa99",
      "metadata": {
        "id": "076cfa99",
        "outputId": "d4bd8f09-932b-40f1-84d6-2ad84b0b0f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Why did the cat sit on the computer? To keep an eye on the mouse!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# send first message\n",
        "user_message_1 = \"Tell me a joke about cats\"\n",
        "session_id_1 = \"convo_1\"\n",
        "chat_with_message_history.invoke({'most_recent_user_message': user_message_1},\n",
        "                                config={\"configurable\": {\"session_id\": session_id_1}})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "441d9331",
      "metadata": {
        "id": "441d9331",
        "outputId": "dd628bd9-0a1e-4065-d27f-0de3beedaf39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The humor comes from the pun on \"mouse,\" referring to both a computer mouse and a rodent that cats hunt.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "user_message_2 = \"What was funny about that joke?\"\n",
        "# send second message\n",
        "chat_with_message_history.invoke({'most_recent_user_message': user_message_2},\n",
        "                                    config={\"configurable\": {\"session_id\": session_id_1}})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets look at the history\n",
        "chat_conversation_threads[session_id_1].messages"
      ],
      "metadata": {
        "id": "BLvEXYRym8Zh",
        "outputId": "c698097b-0d8e-4c4f-f2d2-b56608b1ef24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BLvEXYRym8Zh",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Why did the cat sit on the computer? To keep an eye on the mouse!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What was funny about that joke?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='The humor comes from the pun on \"mouse,\" referring to both a computer mouse and a rodent that cats hunt.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47da336f",
      "metadata": {
        "id": "47da336f"
      },
      "source": [
        "#### View langsmith traces\n",
        "We can take a look at this trace on [Langsmith](https://smith.langchain.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j-T1I1nDiM-A",
      "metadata": {
        "id": "j-T1I1nDiM-A"
      },
      "source": [
        "# Retrieval Augmented Generation (RAG)\n",
        "## Review\n",
        "* Conceptual and step-by-step guide about [RAG](https://python.langchain.com/v0.1/docs/use_cases/question_answering/)\n",
        "* Learn more about implementing [RAG](https://python.langchain.com/docs/expression_language/cookbook/retrieval)\n",
        "\n",
        "**Data Ingestion (Creating a Vector Store of Documents)**\n",
        "<figure>\n",
        "<img src='https://python.langchain.com/v0.1/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png' height=300/>\n",
        "    <figcaption>\n",
        "        Source: Data Ingestion (Preparing Embeddings), from <a href=https://python.langchain.com/v0.1/docs/use_cases/question_answering/>Langchain Use Case: Q&A with RAG</a>\n",
        "    </figcaption>\n",
        "</figure>\n",
        "\n",
        "**Retrieval and Generation**\n",
        "<figure>\n",
        "<img src='https://python.langchain.com/v0.1/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png' height=300/>\n",
        "    <figcaption>\n",
        "        Source: Retrieval and Generation, from <a href=https://python.langchain.com/v0.1/docs/use_cases/question_answering/>Langchain Use Case: Q&A with RAG</a>\n",
        "    </figcaption>\n",
        "</figure>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HOs-D7XQlmzt",
      "metadata": {
        "id": "HOs-D7XQlmzt"
      },
      "source": [
        "## Document Loaders and Splitters\n",
        "[Data Ingestion/Vector Store Preparation Guide ](https://python.langchain.com/docs/modules/data_connection/)\n",
        "<figure>\n",
        "<img src='https://python.langchain.com/v0.1/assets/images/data_connection-95ff2033a8faa5f3ba41376c0f6dd32a.jpg' height=300/>\n",
        "    <figcaption>\n",
        "        Langchain Retrieval Component, from <a href=https://python.langchain.com/docs/modules/data_connection/>Langchain Components</a>\n",
        "    </figcaption>\n",
        "</figure>\n",
        "\n",
        "**Other extremely useful resources**:\n",
        "* **[Components -> Retrieval -> Document Loaders](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/)**: Use the sidebar to navigate through different types of document loaders. For all available integrations available through langchain, see [Components -> Integrations -> Components](https://python.langchain.com/v0.1/docs/integrations/document_loaders/)\n",
        "* **[Components -> Retrieval -> Text Splitters](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/)**: Use the sidebar to navigate through different types of text splitters. For all available integrations available through langchain, see [Components -> Integrations -> Components](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88cf0508",
      "metadata": {
        "id": "88cf0508"
      },
      "outputs": [],
      "source": [
        "# example pdf links\n",
        "doc_1 = 'https://registrar.vanderbilt.edu/documents/Undergraduate_School_Catalog_2023-24_UPDATED2.pdf'\n",
        "doc_2 = 'https://www.tnmd.uscourts.gov/sites/tnmd/files/Pro%20Se%20Nonprisoner%20Handbook.pdf'\n",
        "doc_3 = 'https://www.uscis.gov/sites/default/files/document/guides/M-654.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83aff93e",
      "metadata": {
        "id": "83aff93e"
      },
      "source": [
        "### Example: pdfloader and recursive character splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Sk4B1Ihklod8",
      "metadata": {
        "id": "Sk4B1Ihklod8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b31d1836",
      "metadata": {
        "id": "b31d1836"
      },
      "outputs": [],
      "source": [
        "loader = #choose loader and document\n",
        "\n",
        "# Add the kind of text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=250,\n",
        ")\n",
        "\n",
        "# use the text splitter to split the document\n",
        "chunks = # load and split chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f9490d7",
      "metadata": {
        "id": "1f9490d7"
      },
      "outputs": [],
      "source": [
        "# see how many chunks were made\n",
        "print(len(chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7e3261f",
      "metadata": {
        "id": "c7e3261f"
      },
      "outputs": [],
      "source": [
        "# inspect a single chunk\n",
        "chunks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ab1136",
      "metadata": {
        "id": "08ab1136"
      },
      "outputs": [],
      "source": [
        "# view first 3 chunks\n",
        "for chunk_index, chunk in enumerate(chunks[:3]):\n",
        "    print(f'****** Chunk {chunk_index} ******\\n{chunk.page_content}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1fd5bbb",
      "metadata": {
        "id": "b1fd5bbb"
      },
      "source": [
        "### Example: Loading website data and splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f15b971",
      "metadata": {
        "id": "5f15b971"
      },
      "outputs": [],
      "source": [
        "from bs4 import SoupStrainer\n",
        "from langchain_community.document_loaders import WebBaseLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fa1ea14",
      "metadata": {
        "id": "8fa1ea14"
      },
      "outputs": [],
      "source": [
        "constitution_website = \"https://constitutioncenter.org/the-constitution/full-text\"\n",
        "\n",
        "# load using WebBaseLoader\n",
        "web_loader = WebBaseLoader(constitution_website,\n",
        "                       bs_kwargs = {'parse_only':SoupStrainer(['article'])})\n",
        "\n",
        "# read the document from the website (without splitting)\n",
        "web_document = #load document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c754f1f5",
      "metadata": {
        "id": "c754f1f5"
      },
      "outputs": [],
      "source": [
        "# only the first few characters\n",
        "print(web_document[0].page_content[:330])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "330f96da",
      "metadata": {
        "id": "330f96da"
      },
      "source": [
        "Now, we'll split in a slightly different way. Since we've already scraped the website, we will just directly use the splitter. Note that after we load the website, we have a data type of (list of) `Document`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e15cfb1c",
      "metadata": {
        "id": "e15cfb1c"
      },
      "outputs": [],
      "source": [
        "website_splitter = RecursiveCharacterTextSplitter(chunk_size=330, chunk_overlap=100, # add ability to use start_index\n",
        "website_chunks = website_splitter.split_documents(web_document)\n",
        "len(website_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f581f865",
      "metadata": {
        "id": "f581f865"
      },
      "outputs": [],
      "source": [
        "website_chunks[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "841ab566",
      "metadata": {
        "id": "841ab566"
      },
      "source": [
        "If you know less about the constitution and more about Star wars (or another topic available on Wikipedia), feel free to run the cells below to use that text moving forward. It will replace the `website_chunks` variable. You may need to adjust the `chunk_size` and `chunk_overlap` options. Uncomment and run these cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08bb3ff8",
      "metadata": {
        "id": "08bb3ff8"
      },
      "outputs": [],
      "source": [
        "# alternate data\n",
        "webloader = WebBaseLoader('https://simple.wikipedia.org/wiki/Star_Wars_Episode_IV:_A_New_Hope',\n",
        "                       bs_kwargs = {'parse_only':SoupStrainer('div', id='bodyContent')})\n",
        "web_chunks = webloader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100, add_start_index=True))\n",
        "print('Number of chunks generated: ', len(web_chunks))\n",
        "print('\\n\\nSample: ')\n",
        "web_chunks[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mKdzgo6ci0MD",
      "metadata": {
        "id": "mKdzgo6ci0MD"
      },
      "source": [
        "## Vector Stores: A way to store embeddings (hidden states) of your data\n",
        "The choice of vector store influences how \"relevant\" documents can be identified, speed of document retrieval, and organization.\n",
        "\n",
        "Helpful resources:\n",
        "* **[Brief Langchain Reference](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/)**\n",
        "* **[Vector Store Integrations](https://python.langchain.com/v0.1/docs/integrations/vectorstores/)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2831d1dd",
      "metadata": {
        "id": "2831d1dd"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jmXDTgoqjCUs",
      "metadata": {
        "id": "jmXDTgoqjCUs"
      },
      "outputs": [],
      "source": [
        "# create the vector store\n",
        "db = # code to create store"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "217d27ea",
      "metadata": {
        "id": "217d27ea"
      },
      "source": [
        "### Similarity Search for Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XC-5zEeioFmq",
      "metadata": {
        "id": "XC-5zEeioFmq"
      },
      "outputs": [],
      "source": [
        "# query the vector store\n",
        "query = 'When was a new hope released?'\n",
        "\n",
        "# use a similarity search between the vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d1c197d",
      "metadata": {
        "id": "8d1c197d"
      },
      "outputs": [],
      "source": [
        "# get cosine distance alongside results\n",
        "relevant_docs =\n",
        "relevant_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CWJwyByyhoSB",
      "metadata": {
        "id": "CWJwyByyhoSB"
      },
      "outputs": [],
      "source": [
        "# another query, but instead use normalized score\n",
        "query = 'What is the plot of A New Hope?'\n",
        "relevant_docs = # use normalized score\n",
        "relevant_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lt_uoYfLjUdx",
      "metadata": {
        "id": "lt_uoYfLjUdx"
      },
      "source": [
        "## Retrievers: How we select the most relevant data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KTNxqQf7kiY_",
      "metadata": {
        "id": "KTNxqQf7kiY_"
      },
      "outputs": [],
      "source": [
        "# or use the db as a retriever with lcel\n",
        "retriever = # create retriever\n",
        "retrieved_docs = retriever.invoke(query)\n",
        "retrieved_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8IuaToUPc0Hy",
      "metadata": {
        "id": "8IuaToUPc0Hy"
      },
      "source": [
        "## RAG\n",
        "For when we want to actually do generation, but want there to be retrieved documents included in the generation. For this, we're going to switch to a different embedding model which will be downloaded on our machine (or if on Google Colab, there)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3298219",
      "metadata": {
        "id": "d3298219"
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableSequence, RunnableParallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4f952c1",
      "metadata": {
        "id": "d4f952c1"
      },
      "outputs": [],
      "source": [
        "# use different embedding model\n",
        "embeddings_fn = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") #, model_kwargs={\"device\":'mps'})\n",
        "hf_db = FAISS.from_documents(web_chunks, embeddings_fn)\n",
        "hf_retriever = hf_db.as_retriever(search_kwargs={\"k\":1})\n",
        "\n",
        "# make sure it works\n",
        "query = 'What is the plot of A New Hope?'\n",
        "hf_retriever.invoke(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16cb408b",
      "metadata": {
        "id": "16cb408b"
      },
      "source": [
        "### Default RAG: Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fsewtgf9alyl",
      "metadata": {
        "id": "Fsewtgf9alyl"
      },
      "outputs": [],
      "source": [
        "# Basic question answering template\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# compose prompt\n",
        "rag_prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# create model (so we don't have to depend on the model definition at the top of the notebook)\n",
        "model = ChatOpenAI(model_name='gpt-3.5-turbo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ExSC1rNNmCw",
      "metadata": {
        "id": "8ExSC1rNNmCw"
      },
      "outputs": [],
      "source": [
        "# We need to format the retrieved documents better\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([f'Reference text:\\n{doc.page_content}\\n\\Citation Info: {doc.metadata}' for doc in docs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73636824",
      "metadata": {
        "id": "73636824"
      },
      "outputs": [],
      "source": [
        "# inspect behavior of format_docs\n",
        "format_docs(web_chunks[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21071aec",
      "metadata": {
        "id": "21071aec"
      },
      "outputs": [],
      "source": [
        "# compose the chain\n",
        "rag_chain = (\n",
        "    ## Add special rag part\n",
        "    | rag_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2cba6e1",
      "metadata": {
        "id": "e2cba6e1"
      },
      "outputs": [],
      "source": [
        "# run the chain\n",
        "rag_chain.with_config(run_name = 'basic_rag_chain').invoke('What is the plot of a new hope')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31c99c95",
      "metadata": {
        "id": "31c99c95"
      },
      "source": [
        "### RAG with Sources\n",
        "Resource: [Langchain: Returning Sources](https://python.langchain.com/v0.1/docs/use_cases/question_answering/sources/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d47c733e",
      "metadata": {
        "id": "d47c733e"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableParallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fdb371f",
      "metadata": {
        "id": "7fdb371f"
      },
      "outputs": [],
      "source": [
        "# Basic prompt -> model -> parser chain\n",
        "single_turn_chain = (\n",
        "    rag_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Break previous chain in half to access context and question in returned response\n",
        "rag_chain_with_source = RunnableParallel(\n",
        "    {\"context\": hf_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        ").assign(answer=single_turn_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4b36d6e",
      "metadata": {
        "id": "e4b36d6e"
      },
      "outputs": [],
      "source": [
        "# invoke\n",
        "response = rag_chain_with_source.with_config(run_name = 'sources_rag_chain').invoke(\"What happened to Princess Leia in a New Hope?\")\n",
        "\n",
        "# print full response\n",
        "for key, value in response.items():\n",
        "    print(f\"{key}: {value}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afec178a",
      "metadata": {
        "id": "afec178a"
      },
      "source": [
        "### RAG with Chat History?\n",
        "\n",
        "We will have a one-turn system with our RAG system. How do we add chat memory? See below for implementation guides:\n",
        "- [Use cases: Q&A with Rag: Add Chat History.](https://python.langchain.com/v0.1/docs/use_cases/question_answering/chat_history/)  Builds on a RAG system, so will be of interest."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9d3434e",
      "metadata": {
        "id": "a9d3434e"
      },
      "source": [
        "## LLM System Metrics\n",
        "Resource: [Guides -> Evaluation](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92372d35",
      "metadata": {
        "id": "92372d35"
      },
      "outputs": [],
      "source": [
        "from langchain.evaluation import load_evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7bac358",
      "metadata": {
        "id": "c7bac358"
      },
      "outputs": [],
      "source": [
        "# configure what we want to evaluate\n",
        "rs_question = \"What happened to Princess Leia in a New Hope?\"\n",
        "rs_answer = rag_chain.with_config(run_name = 'basic_rag_chain').invoke(rs_question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da42e15",
      "metadata": {
        "id": "1da42e15"
      },
      "outputs": [],
      "source": [
        "# load an evaluator that uses the conciseness criteria\n",
        "evaluator = #load evaluator\n",
        "\n",
        "# evaluate whether our model was concise or not\n",
        "eval_result = evaluator.evaluate_strings(\n",
        "    # add inputs to evaluate\n",
        ")\n",
        "\n",
        "# print result\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "933a5e2e",
      "metadata": {
        "id": "933a5e2e"
      },
      "source": [
        "View other criteria available through LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6dbb810",
      "metadata": {
        "id": "e6dbb810"
      },
      "outputs": [],
      "source": [
        "from langchain.evaluation import Criteria\n",
        "list(Criteria)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34ea2044",
      "metadata": {
        "id": "34ea2044"
      },
      "source": [
        "# Homework\n",
        "The following exercises are designed to help you gain depth in what you've learned about RAG today.\n",
        "\n",
        "## [Required] Learning more about RAG\n",
        "### Splitting Text (Conceptual)\n",
        "There are so many ways to split the text, and each has an impact on the resultant RAG system. Below is a resource (with sidebar dropdown) for you to read over and then answer the following question for the text splitting approaches (as relevant to your application):\n",
        "* What is the proposed value in adopting this text splitting approach? What are some drawbacks?\n",
        "\n",
        "### Splitting Text (Programmatic)\n",
        "Above, we have adopted specific chunk sizes and splitting approaches. Choose one of the documents (or use your own) and:\n",
        "* Modify the chunk size. How does this impact the resulting RAG performance? The cost?\n",
        "* Implement a different type of text splitter (as applicable, i.e., not code text splitters if you're not splitting code). How does this impact the resulting RAG performance? The cost?\n",
        "\n",
        "### Customizing RAG\n",
        "There are many, many, many ways to improve results with RAG. Below are some resources for you to read over then complete the following:\n",
        "1. What is the proposed value in adopting this approach? In other words, what is the expected performance improvement by using this method?\n",
        "2. How might it apply to your work?\n",
        "\n",
        "* [**Query Analysis**](https://python.langchain.com/v0.1/docs/use_cases/query_analysis/). Make sure to peruse subtopics.\n",
        "* [**Synthetic Data Generation**](https://python.langchain.com/v0.1/docs/use_cases/data_generation/).\n",
        "* [**Tagging**](https://python.langchain.com/v0.1/docs/use_cases/tagging/).\n",
        "* [**Routing Chain Logic Based on inputs****](https://python.langchain.com/v0.1/docs/expression_language/how_to/routing/).\n",
        "* [**Chain Composition**](https://python.langchain.com/v0.1/docs/modules/chains/). Of particular interest here are the Legacy chains. Although they will probably be completely removed in the future, consider their behavior. In what cases might these behaviors be useful?\n",
        "\n",
        "** This is highly recommended reading, but may not be suitable for those who are novices in programming. Although there is text, the code demonstrates concretely by the text. For novices, it may be better to copy/paste the code as well to understand the behavior, although it is noted that such a task may be outside of the the time constraints of for some participants.\n",
        "\n",
        "## [Required] Learning more about Evaluation\n",
        "Read the following text and answer these questions:\n",
        "1. What is the purpose of the individual criterion? Does it require and external LLM for evaluation?\n",
        "2. In what cases might this criteria be useful?\n",
        "\n",
        "Depth Text: [**Evaluation, by Langchain**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/)\n",
        "\n",
        "## [Highly recommended] Learning more about the LLM System Lifecycle\n",
        "There is more to an LLM-based system than a user interface and the LLM chain. There is a whole framework around inspecting, testing, and evaluating these systems. Read the following and answer the questions below:\n",
        "1. Summarize the purpose of the individual components of the langsmith system (they generalize to all LLM systems).\n",
        "2. Consider your favorite LLM UI (i.e., ChatGPT, Gemini, Claude, etc). Describe how you think these components are utilized the LLM system.\n",
        "\n",
        "Depth Text: [**LangSmith User Guide**](https://docs.smith.langchain.com/old/user_guide)\n",
        "\n",
        "## [Recommended] Practicing with RAG and Langchain\n",
        "### Exercise 1: Modify the RAG system\n",
        "Modify or create a new chain which:\n",
        "1. Uses a different LLM than the one used in this notebook.\n",
        "2. Uses a different document loader\n",
        "3. Uses a different splitter than the one used in this notebook.\n",
        "4. Uses a different vector store/retriever than the one used in this notebook.\n",
        "\n",
        "Use the resources provided in the relevant sections of the notebook for other options.\n",
        "\n",
        "### Exercise 2: Implement a new RAG system\n",
        "1. [More challenging] Add chat history to one of your RAG chains. Make sure to enable tracing and inspect langsmith to ensure that the chat history is used.\n",
        "2. Create a gradio user interface to use your chain in a more user-friendly way.\n",
        "3. [Challenging] Implement an additional chain which uses one of the strategies you read about in the \"Learning more about RAG\" section.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}